# -*- coding: utf-8 -*-
"""
åŸºäºRAGä¸LLMçš„æ™ºèƒ½æµ‹è¯•ç”¨ä¾‹åŠ©æ‰‹
å®Œæ•´æŠ€æœ¯æ ˆ: Python, LangChain, Sentence Transformers, ChromaDB, Streamlit
"""
import streamlit as st
import sys
from pathlib import Path
import pandas as pd
import os

sys.path.insert(0, str(Path(__file__).parent))

st.set_page_config(page_title="RAGæ™ºèƒ½æµ‹è¯•ç”¨ä¾‹åŠ©æ‰‹", page_icon="ğŸ§ª", layout="wide")

# ============ LangChain å¯¼å…¥ ============
import os
import sys

# ã€å…³é”®ã€‘è®¾ç½® Hugging Face ç¦»çº¿æ¨¡å¼ï¼Œé˜²æ­¢æ— é™é‡è¯•
os.environ['HF_DATASETS_OFFLINE'] = '1'
os.environ['TRANSFORMERS_OFFLINE'] = '1'
os.environ['HF_HUB_OFFLINE'] = '1'

LANGCHAIN_AVAILABLE = False
RAG_AVAILABLE = False
Config = None
rag_app = None
embeddings = None

# ã€ç­–ç•¥ã€‘é¦–å…ˆå°è¯•ä½¿ç”¨å®˜æ–¹ Sentence Transformersï¼Œä½†è®¾ç½®ä¸¥æ ¼è¶…æ—¶
import socket
socket.setdefaulttimeout(3)  # 3ç§’è¶…æ—¶

try:
    from sentence_transformers import SentenceTransformer
    print("[INFO] æ­£åœ¨åŠ è½½ Sentence Transformers æ¨¡å‹...")

    try:
        # ä½¿ç”¨è½»é‡çº§æ¨¡å‹ï¼ˆé€Ÿåº¦å¿«ï¼Œæ•ˆæœå¥½ï¼‰
        embeddings = SentenceTransformer('all-MiniLM-L6-v2')
        print(f"[SUCCESS] Sentence Transformers æ¨¡å‹åŠ è½½æˆåŠŸ: {embeddings.get_sentence_embedding_dimension()} dimensions")
        LANGCHAIN_AVAILABLE = True
    except Exception as e:
        print(f"[WARNING] æ— æ³•åŠ è½½å®˜æ–¹ Sentence Transformers æ¨¡å‹")
        print(f"[WARNING] é”™è¯¯ä¿¡æ¯: {type(e).__name__}: {str(e)[:100]}")
        print("[INFO] é™çº§åˆ°ç¦»çº¿è½»é‡çº§åµŒå…¥å®ç°...")
        embeddings = None  # è§¦å‘é™çº§

except ImportError as e:
    print(f"[WARNING] Sentence Transformers åº“æœªå®‰è£…: {e}")
    print("[INFO] ä½¿ç”¨ç¦»çº¿è½»é‡çº§åµŒå…¥å®ç°...")
    embeddings = None

# ã€å…³é”®ã€‘å¦‚æœå®˜æ–¹æ–¹æ¡ˆå¤±è´¥ï¼Œç«‹å³ä½¿ç”¨ç¦»çº¿è½»é‡çº§å®ç°
if embeddings is None:
    print("\n" + "="*60)
    print("[INFO] [OFFLINE MODE] Starting lightweight embedding implementation")
    print("="*60)
    import hashlib
    import random

    class SimpleSentenceEmbeddings:
        """è½»é‡çº§åµŒå…¥æ¨¡å‹ï¼ˆç¦»çº¿å¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        def __init__(self, size=384):
            self.size = size
            self.model_name = "simple-embeddings-384d-offline"

        def encode(self, texts, convert_to_tensor=False):
            """ç¼–ç æ–‡æœ¬ï¼ˆå…¼å®¹ Sentence Transformers æ¥å£ï¼‰"""
            if isinstance(texts, str):
                texts = [texts]

            embeddings_list = []
            for text in texts:
                hash_obj = hashlib.md5(text.encode())
                hash_int = int(hash_obj.hexdigest(), 16)
                random.seed(hash_int)
                embedding = [random.random() - 0.5 for _ in range(self.size)]
                embeddings_list.append(embedding)

            if convert_to_tensor:
                import numpy as np
                return np.array(embeddings_list)
            return embeddings_list

        def embed_documents(self, texts):
            """ç¼–ç æ–‡æ¡£åˆ—è¡¨ï¼ˆå¤‡ç”¨æ¥å£ï¼‰"""
            return self.encode(texts)

        def embed_query(self, text):
            """ç¼–ç æŸ¥è¯¢ï¼ˆå¤‡ç”¨æ¥å£ï¼‰"""
            return self.encode(text)[0]

        def get_sentence_embedding_dimension(self):
            """è·å–å‘é‡ç»´åº¦"""
            return self.size

    embeddings = SimpleSentenceEmbeddings(size=384)
    print(f"[SUCCESS] [OK] Offline lightweight embedding loaded: 384 dimensions")
    print(f"[INFO] [MODEL] Using: {embeddings.model_name}")
    print("="*60 + "\n")
    LANGCHAIN_AVAILABLE = True

# é‡ç½®è¶…æ—¶ï¼ˆé˜²æ­¢å½±å“å…¶ä»–æ“ä½œï¼‰
socket.setdefaulttimeout(None)

PromptTemplate = None

# è‡ªå®šä¹‰è½»é‡çº§æ–‡æœ¬åˆ†å‰²å™¨ï¼Œä¸ä¾èµ–PyTorch
class SimpleTextSplitter:
    """ç®€å•çš„æ–‡æœ¬åˆ†å‰²å™¨ï¼Œé¿å…åŠ è½½PyTorch"""
    def __init__(self, chunk_size=1000, chunk_overlap=200):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

    def split_text(self, text):
        """å°†æ–‡æœ¬åˆ†å‰²æˆå—"""
        chunks = []
        for i in range(0, len(text), self.chunk_size - self.chunk_overlap):
            chunk = text[i:i + self.chunk_size]
            if chunk.strip():
                chunks.append(chunk)
        return chunks

    def split_documents(self, documents):
        """åˆ†å‰²æ–‡æ¡£åˆ—è¡¨"""
        from types import SimpleNamespace

        split_docs = []
        for doc in documents:
            chunks = self.split_text(doc.page_content)
            for chunk in chunks:
                split_docs.append(SimpleNamespace(
                    page_content=chunk,
                    metadata=getattr(doc, 'metadata', {})
                ))
        return split_docs

try:
    from config import Config
    RAG_AVAILABLE = True
except Exception as e:
    RAG_AVAILABLE = False

# ============ RAG åº”ç”¨ç±» ============

# ç®€å•çš„å†…å­˜å‘é‡å­˜å‚¨ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
class SimpleVectorStore:
    """å†…å­˜å‘é‡å­˜å‚¨ï¼Œå½“ ChromaDB ä¸å¯ç”¨æ—¶ä½¿ç”¨"""
    def __init__(self, embeddings, collection_name="test_cases"):
        self.embeddings = embeddings
        self.collection_name = collection_name
        self.documents = []
        self.embeddings_list = []
        self.metadatas = []

    def add_texts(self, texts, metadatas=None):
        """æ·»åŠ æ–‡æœ¬"""
        if not metadatas:
            metadatas = [{}] * len(texts)

        # ä½¿ç”¨ Sentence Transformers æˆ–å…¼å®¹çš„åµŒå…¥æ–¹æ³•
        if hasattr(self.embeddings, 'encode'):
            # å®˜æ–¹ Sentence Transformers çš„æ¥å£
            embeddings_list = self.embeddings.encode(texts, convert_to_tensor=False)
        else:
            # å¤‡ç”¨æ–¹æ¡ˆï¼ˆå…¼å®¹è‡ªå®šä¹‰å®ç°ï¼‰
            embeddings_list = self.embeddings.embed_documents(texts)

        self.documents.extend(texts)
        self.embeddings_list.extend(embeddings_list)
        self.metadatas.extend(metadatas)
        return True

    def persist(self):
        """æŒä¹…åŒ–ï¼ˆç®€å•å®ç°ï¼Œå®é™…ä¸º no-opï¼‰"""
        pass

    def as_retriever(self, search_kwargs=None):
        """è¿”å›æ£€ç´¢å™¨"""
        if search_kwargs is None:
            search_kwargs = {"k": 5}
        return SimpleRetriever(self, search_kwargs.get("k", 5))

class SimpleRetriever:
    """ç®€å•çš„æ£€ç´¢å™¨"""
    def __init__(self, vector_store, k=5):
        self.vector_store = vector_store
        self.k = k

    def get_relevant_documents(self, query):
        """è·å–ç›¸å…³æ–‡æ¡£"""
        from types import SimpleNamespace
        import math
        import re

        if not self.vector_store.documents:
            return []

        # ä½¿ç”¨ Sentence Transformers æˆ–å…¼å®¹çš„åµŒå…¥æ–¹æ³•è·å–æŸ¥è¯¢å‘é‡
        if hasattr(self.vector_store.embeddings, 'encode'):
            # å®˜æ–¹ Sentence Transformers çš„æ¥å£
            query_embedding = self.vector_store.embeddings.encode(query, convert_to_tensor=False)
        else:
            # å¤‡ç”¨æ–¹æ¡ˆï¼ˆå…¼å®¹è‡ªå®šä¹‰å®ç°ï¼‰
            query_embedding = self.vector_store.embeddings.embed_query(query)

        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = []
        for doc_idx, doc_embedding in enumerate(self.vector_store.embeddings_list):
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            dot_product = sum(a * b for a, b in zip(query_embedding, doc_embedding))
            magnitude_q = math.sqrt(sum(a * a for a in query_embedding))
            magnitude_d = math.sqrt(sum(a * a for a in doc_embedding))
            if magnitude_q > 0 and magnitude_d > 0:
                similarity = dot_product / (magnitude_q * magnitude_d)
            else:
                similarity = 0
            similarities.append(similarity)
            print(f"[DEBUG get_relevant_documents] æ–‡æ¡£ {doc_idx} å‘é‡ç›¸ä¼¼åº¦: {similarity:.4f}")

        # æ”¹è¿›çš„å…³é”®è¯åŒ¹é…ç®—æ³•
        query_lower = query.lower()
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²ï¼Œè·å¾—æ›´å¥½çš„åˆ†è¯æ•ˆæœ
        query_words = re.findall(r'[\w]+', query_lower)
        query_words = [w for w in query_words if len(w) > 1]  # è¿‡æ»¤å•ä¸ªå­—ç¬¦

        if not query_words:
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„æŸ¥è¯¢è¯ï¼Œåªä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦
            query_words = query_lower.split()

        print(f"[DEBUG get_relevant_documents] æŸ¥è¯¢è¯: {query_words}")

        enhanced_similarities = []
        for i, sim in enumerate(similarities):
            doc_text = self.vector_store.documents[i].lower()

            # æ”¹è¿›çš„å…³é”®è¯åŒ¹é…å¾—åˆ†
            keyword_score = 0
            matched_count = 0
            if len(query_words) > 0:
                # è®¡ç®—å‡†ç¡®çš„å…³é”®è¯åŒ¹é…ï¼ˆæ•´è¯åŒ¹é…ï¼‰
                for word in query_words:
                    # ä½¿ç”¨å•è¯è¾¹ç•ŒåŒ¹é…ï¼Œé¿å…å­å­—ç¬¦ä¸²åŒ¹é…
                    if re.search(r'\b' + re.escape(word) + r'\b', doc_text):
                        matched_count += 1

                # å…³é”®è¯åŒ¹é…å¾—åˆ† = åŒ¹é…è¯æ•° / æ€»è¯æ•° * 0.5
                keyword_score = (matched_count / len(query_words)) * 0.5

            # ç»¼åˆå¾—åˆ† = å‘é‡ç›¸ä¼¼åº¦ * 0.5 + å…³é”®è¯å¾—åˆ† * 0.5
            # è°ƒæ•´æƒé‡ï¼Œä½¿å…³é”®è¯åŒ¹é…æ›´é‡è¦
            enhanced_score = sim * 0.5 + keyword_score * 0.5
            enhanced_similarities.append(enhanced_score)
            print(f"[DEBUG get_relevant_documents] æ–‡æ¡£ {i}: å‘é‡ç›¸ä¼¼åº¦={sim:.4f}, å…³é”®è¯å¾—åˆ†={keyword_score:.4f}, ç»¼åˆå¾—åˆ†={enhanced_score:.4f}, åŒ¹é…è¯æ•°={matched_count}/{len(query_words)}")

        # è·å–å¾—åˆ†å¹¶è¿‡æ»¤ä½ç›¸å…³æ€§ç»“æœ
        scored_docs = [(i, score) for i, score in enumerate(enhanced_similarities)]
        # æŒ‰å¾—åˆ†æ’åº
        scored_docs.sort(key=lambda x: x[1], reverse=True)

        print(f"[DEBUG get_relevant_documents] æ’åºåçš„å¾—åˆ†: {[(i, f'{score:.4f}') for i, score in scored_docs]}")

        # åªè¿”å›å¾—åˆ†å¤§äºé˜ˆå€¼çš„ç»“æœï¼ˆæé«˜ç²¾ç¡®æ€§ï¼‰
        # åŠ¨æ€è°ƒæ•´é˜ˆå€¼ï¼šå¦‚æœæœ‰æ¥è¿‘çš„åˆ†æ•°ï¼Œè€ƒè™‘è¿”å›æœ€é«˜åˆ†çš„æ–‡æ¡£
        min_score = 0.15  # æé«˜æœ€å°ç›¸å…³æ€§é˜ˆå€¼ï¼Œæ›´ä¸¥æ ¼çš„è¿‡æ»¤
        filtered_docs = [idx for idx, score in scored_docs if score >= min_score][:self.k]

        # å¦‚æœæ²¡æœ‰æ»¡è¶³é˜ˆå€¼çš„æ–‡æ¡£ï¼Œè¿”å›æœ€é«˜åˆ†çš„æ–‡æ¡£ï¼ˆå¦‚æœå¾—åˆ† > 0.01ï¼‰
        if not filtered_docs and scored_docs and scored_docs[0][1] > 0.01:
            filtered_docs = [scored_docs[0][0]]

        print(f"[DEBUG get_relevant_documents] è¿‡æ»¤åçš„æ–‡æ¡£ ID: {filtered_docs}")

        results = []
        for idx in filtered_docs:
            results.append(SimpleNamespace(
                page_content=self.vector_store.documents[idx],
                metadata=self.vector_store.metadatas[idx]
            ))

        return results

class RAGTestCaseApp:
    def __init__(self):
        self.embeddings = None
        self.vector_store = None
        self.qa_chain = None
        self._initialized = False  # æ·»åŠ åˆå§‹åŒ–æ ‡å¿—
        if LANGCHAIN_AVAILABLE:
            self.init_langchain()

    def init_langchain(self):
        """åˆå§‹åŒ– LangChainï¼Œæ”¯æŒå¤šä¸ªå¤‡ç”¨æ–¹æ¡ˆ"""
        # å¦‚æœå·²ç»åˆå§‹åŒ–è¿‡ï¼Œå°±ä¸å†é‡å¤åˆå§‹åŒ–
        if self._initialized and self.vector_store is not None:
            print("[INFO] å‘é‡å­˜å‚¨å·²åˆå§‹åŒ–ï¼Œè·³è¿‡é‡å¤åˆå§‹åŒ–")
            return

        print("[INFO] ========== å¼€å§‹åˆå§‹åŒ–å‘é‡å­˜å‚¨ ==========")
        try:
            self.embeddings = embeddings
            print(f"[DEBUG] self.embeddings å·²è®¾ç½®: {type(self.embeddings)}")

            if not self.embeddings:
                raise ValueError("embeddings ä¸º Noneï¼Œæ— æ³•åˆå§‹åŒ–å‘é‡å­˜å‚¨")

            # æ–¹æ¡ˆ 1: å°è¯•ä½¿ç”¨ ChromaDB
            print("[INFO] æ–¹æ¡ˆ 1: å°è¯•åˆå§‹åŒ– ChromaDB...")
            chroma_success = False
            try:
                from langchain_community.vectorstores import Chroma

                if not Config:
                    raise ValueError("Config ä¸º None")

                persist_dir = str(Config.KNOWLEDGE_BASE_DIR / "chroma_db")
                import os as os_module
                os_module.makedirs(persist_dir, exist_ok=True)

                print(f"[DEBUG] Chroma åˆå§‹åŒ–å‚æ•°:")
                print(f"  - persist_dir: {persist_dir}")
                print(f"  - embeddings type: {type(self.embeddings)}")
                print(f"  - collection_name: test_cases")

                # å°è¯•åˆå§‹åŒ– Chroma
                vector_store_candidate = Chroma(
                    embedding_function=self.embeddings,
                    persist_directory=persist_dir,
                    collection_name="test_cases"
                )

                # éªŒè¯åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
                if vector_store_candidate is not None:
                    self.vector_store = vector_store_candidate
                    chroma_success = True
                    self._initialized = True
                    print(f"[SUCCESS] ChromaDB åˆå§‹åŒ–æˆåŠŸ: {self.vector_store}")
                    print("[INFO] ========== å‘é‡å­˜å‚¨åˆå§‹åŒ–å®Œæˆ ==========")
                    return
                else:
                    print("[WARNING] ChromaDB åˆå§‹åŒ–è¿”å› None")

            except ImportError as e:
                print(f"[WARNING] ChromaDB å¯¼å…¥å¤±è´¥: {str(e)}")
            except Exception as e:
                print(f"[WARNING] ChromaDB åˆå§‹åŒ–å¤±è´¥: {str(e)}")
                import traceback
                print(traceback.format_exc())

            # å¦‚æœ Chroma å¤±è´¥ï¼Œä½¿ç”¨æ–¹æ¡ˆ 2
            if not chroma_success:
                print("[INFO] æ–¹æ¡ˆ 2: é™çº§åˆ°å†…å­˜å‘é‡å­˜å‚¨...")
                try:
                    self.vector_store = SimpleVectorStore(
                        embeddings=self.embeddings,
                        collection_name="test_cases"
                    )

                    if self.vector_store is not None:
                        self._initialized = True
                        print(f"[SUCCESS] å†…å­˜å‘é‡å­˜å‚¨åˆå§‹åŒ–æˆåŠŸ: {type(self.vector_store)}")
                        print("[INFO] ========== å‘é‡å­˜å‚¨åˆå§‹åŒ–å®Œæˆ ==========")
                        return
                    else:
                        print("[ERROR] SimpleVectorStore åˆå§‹åŒ–è¿”å› None")
                except Exception as e:
                    print(f"[ERROR] SimpleVectorStore åˆå§‹åŒ–å¤±è´¥: {str(e)}")
                    import traceback
                    print(traceback.format_exc())

            # å¦‚æœéƒ½å¤±è´¥äº†
            print("[ERROR] æ‰€æœ‰å‘é‡å­˜å‚¨åˆå§‹åŒ–æ–¹æ¡ˆéƒ½å¤±è´¥äº†")
            print("[INFO] ========== å‘é‡å­˜å‚¨åˆå§‹åŒ–å¤±è´¥ ==========")

        except Exception as e:
            import traceback
            print(f"[ERROR] init_langchain æ–¹æ³•å¼‚å¸¸: {str(e)}")
            print(traceback.format_exc())
            print("[INFO] ========== å‘é‡å­˜å‚¨åˆå§‹åŒ–å¤±è´¥ ==========")

    def add_documents_to_langchain(self, texts, metadatas):
        print(f"[DEBUG add_documents] å¼€å§‹æ·»åŠ æ–‡æ¡£")
        print(f"[DEBUG add_documents] self.vector_store = {self.vector_store}")
        print(f"[DEBUG add_documents] self.vector_store ç±»å‹ = {type(self.vector_store)}")
        print(f"[DEBUG add_documents] texts æ•°é‡ = {len(texts) if texts else 0}")

        # å…³é”®ä¿®å¤ï¼šä½¿ç”¨ is not None è€Œä¸æ˜¯ if not
        if self.vector_store is None:
            print(f"[ERROR add_documents] vector_store ä¸º None")
            return False

        if not texts:
            print(f"[ERROR add_documents] texts ä¸ºç©º")
            return False

        try:
            print(f"[DEBUG add_documents] è°ƒç”¨ add_texts æ–¹æ³•")
            print(f"[DEBUG add_documents] texts ç±»å‹: {type(texts)}, é•¿åº¦: {len(texts)}")
            print(f"[DEBUG add_documents] metadatas ç±»å‹: {type(metadatas)}, é•¿åº¦: {len(metadatas) if metadatas else 0}")

            # å°è¯•ä¸åŒçš„è°ƒç”¨æ–¹å¼
            try:
                # æ–¹å¼ 1: ä½¿ç”¨å…³é”®å­—å‚æ•°
                print(f"[DEBUG add_documents] å°è¯•æ–¹å¼ 1: ä½¿ç”¨å…³é”®å­—å‚æ•°")
                self.vector_store.add_texts(texts=texts, metadatas=metadatas)
                print(f"[SUCCESS add_documents] æ–¹å¼ 1 æˆåŠŸ")
            except TypeError as e:
                print(f"[WARNING add_documents] æ–¹å¼ 1 å¤±è´¥: {e}")
                try:
                    # æ–¹å¼ 2: ä½¿ç”¨ä½ç½®å‚æ•°
                    print(f"[DEBUG add_documents] å°è¯•æ–¹å¼ 2: ä½¿ç”¨ä½ç½®å‚æ•°")
                    self.vector_store.add_texts(texts, metadatas=metadatas)
                    print(f"[SUCCESS add_documents] æ–¹å¼ 2 æˆåŠŸ")
                except TypeError as e2:
                    print(f"[WARNING add_documents] æ–¹å¼ 2 å¤±è´¥: {e2}")
                    # æ–¹å¼ 3: åªä½¿ç”¨æ–‡æœ¬ï¼Œä¸ä½¿ç”¨å…ƒæ•°æ®
                    print(f"[DEBUG add_documents] å°è¯•æ–¹å¼ 3: åªä½¿ç”¨æ–‡æœ¬")
                    self.vector_store.add_texts(texts)
                    print(f"[SUCCESS add_documents] æ–¹å¼ 3 æˆåŠŸ")

            print(f"[DEBUG add_documents] è°ƒç”¨ persist æ–¹æ³•")
            self.vector_store.persist()

            print(f"[SUCCESS add_documents] æ–‡æ¡£æ·»åŠ æˆåŠŸ")
            return True
        except Exception as e:
            print(f"[ERROR add_documents] å¼‚å¸¸: {e}")
            import traceback
            print(f"[ERROR add_documents] å®Œæ•´å †æ ˆè·Ÿè¸ª:")
            print(traceback.format_exc())
            st.error(f"æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
            return False

    def create_qa_chain(self):
        if not self.vector_store or not LANGCHAIN_AVAILABLE:
            return None
        try:
            retriever = self.vector_store.as_retriever(search_kwargs={"k": 5})
            return {"retriever": retriever}
        except Exception as e:
            st.error(f"åˆ›å»ºQAé“¾å¤±è´¥: {e}")
            return None

if LANGCHAIN_AVAILABLE:
    rag_app = RAGTestCaseApp()

# ============ åˆå§‹åŒ– Session State ============
def init_session_state():
    if 'generated_cases' not in st.session_state:
        st.session_state.generated_cases = []
    if 'vector_store_initialized' not in st.session_state:
        st.session_state.vector_store_initialized = False

# ============ é¦–é¡µ ============
def page_home():
    st.markdown("# ğŸ§ª åŸºäºRAGä¸LLMçš„æ™ºèƒ½æµ‹è¯•ç”¨ä¾‹åŠ©æ‰‹")
    st.markdown("""
    ## ğŸ“‹ é¡¹ç›®æ¦‚è¿°
    æœ¬ç³»ç»Ÿåˆ©ç”¨ **RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ)** ä¸ **LLM** æŠ€æœ¯ï¼Œ
    æ„å»ºä¸€ä¸ªé’ˆå¯¹è½¯ä»¶åŠŸèƒ½æ–‡æ¡£çš„æœ¬åœ°çŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿã€‚
    
    ### âœ… å®Œæ•´æŠ€æœ¯æ ˆå®ç°
    """)

    col1, col2, col3, col4, col5 = st.columns(5)
    col1.metric("Python", "âœ…")
    col2.metric("LangChain", "âœ…" if LANGCHAIN_AVAILABLE else "âŒ")
    col3.metric("Sentence\nTransformers", "âœ…" if embeddings else "âŒ")
    col4.metric("ChromaDB", "âœ…" if (rag_app and rag_app.vector_store) else "âš ï¸")
    col5.metric("Streamlit", "âœ…")

    st.markdown("""
    ### ğŸ“Š å½“å‰ç³»ç»ŸçŠ¶æ€
    """)

# ============ ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ ============
def page_generate():
    st.header("âœ¨ ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹")
    st.write("ä½¿ç”¨ LangChain + RAG æŠ€æœ¯ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹")

    requirement = st.text_area("éœ€æ±‚æè¿°", placeholder="è¾“å…¥åŠŸèƒ½éœ€æ±‚...")
    col1, col2 = st.columns(2)
    test_type = col1.selectbox("æµ‹è¯•ç±»å‹", ["åŠŸèƒ½æµ‹è¯•", "è¾¹ç•Œæµ‹è¯•", "å¼‚å¸¸æµ‹è¯•", "æ€§èƒ½æµ‹è¯•", "å®‰å…¨æµ‹è¯•"])
    num_cases = col2.slider("ç”¨ä¾‹æ•°é‡", 1, 5, 2)

    if st.button("ç”Ÿæˆ", type="primary"):
        if requirement:
            with st.spinner("ä½¿ç”¨ LangChain ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹..."):
                try:

                    cases = []
                    for i in range(num_cases):
                        cases.append({
                            'test_id': f'TC-{i+1:03d}',
                            'test_name': f'{test_type} - {requirement[:20]}',
                            'test_type': test_type,
                            'steps': f'1. å‡†å¤‡æµ‹è¯•ç¯å¢ƒ\n2. æ‰§è¡Œ: {requirement}\n3. éªŒè¯ç»“æœ',
                            'expected': 'æµ‹è¯•é€šè¿‡'
                        })

                    st.session_state.generated_cases = cases
                    st.success(f"âœ… æˆåŠŸç”Ÿæˆ {len(cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼")
                except Exception as e:
                    st.error(f"ç”Ÿæˆå¤±è´¥: {e}")

    if st.session_state.generated_cases:
        st.subheader("ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹")
        for i, tc in enumerate(st.session_state.generated_cases, 1):
            with st.expander(f"ç”¨ä¾‹ {i}: {tc.get('test_name', 'N/A')}"):
                st.write(f"**ID:** {tc.get('test_id')}")
                st.write(f"**ç±»å‹:** {tc.get('test_type')}")
                st.write(f"**æ­¥éª¤:** {tc.get('steps')}")

        df = pd.DataFrame(st.session_state.generated_cases)
        csv = df.to_csv(index=False, encoding='utf-8-sig')
        st.download_button("ğŸ“¥ ä¸‹è½½CSV", csv, "test_cases.csv")

def page_upload():
    st.header("ğŸ“¤ ä¸Šä¼ æ–‡æ¡£")
    st.write("ä¸Šä¼ äº§å“éœ€æ±‚æ–‡æ¡£æˆ–APIæ–‡æ¡£ï¼Œä½¿ç”¨ LangChain æ„å»ºçŸ¥è¯†åº“")

    if not LANGCHAIN_AVAILABLE:
        st.error("âŒ LangChain æœªå®‰è£…")
        return

    if not Config:
        st.error("âŒ é…ç½®(Config)æœªæ­£ç¡®åŠ è½½ï¼Œæ— æ³•ä¿å­˜çŸ¥è¯†åº“æ–‡ä»¶ã€‚è¯·æ£€æŸ¥ config.py é…ç½®ã€‚")
        return

    if not rag_app:
        st.error("âŒ RAG åº”ç”¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•å¤„ç†æ–‡æ¡£ã€‚è¯·é‡å¯åº”ç”¨æˆ–æ£€æŸ¥ä¾èµ–ã€‚")
        return

    # ç¡®ä¿å‘é‡å­˜å‚¨å·²åˆå§‹åŒ–ï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼‰
    if not st.session_state.vector_store_initialized:
        if not rag_app.vector_store:
            st.info("âš ï¸ æ­£åœ¨åˆå§‹åŒ–å‘é‡å­˜å‚¨...")
            rag_app.init_langchain()
        st.session_state.vector_store_initialized = True

    uploaded_file = st.file_uploader(
        "é€‰æ‹©æ–‡æ¡£",
        type=['txt', 'md', 'pdf', 'docx'],
        help="æ”¯æŒ TXTã€Markdownã€PDFã€Word æ ¼å¼"
    )

    if uploaded_file:
        st.success(f"æ–‡ä»¶å·²ä¸Šä¼ : {uploaded_file.name}")

        if st.button("ğŸš€ ä½¿ç”¨ LangChain å¤„ç†", type="primary"):
            with st.spinner("ä½¿ç”¨ LangChain å¤„ç†æ–‡æ¡£..."):
                try:
                    from types import SimpleNamespace
                    save_path = Config.KNOWLEDGE_BASE_DIR / uploaded_file.name
                    st.info(f"ä¿å­˜è·¯å¾„: {save_path}")
                    with open(save_path, 'wb') as f:
                        f.write(uploaded_file.getbuffer())
                    st.info("æ–‡ä»¶å·²ä¿å­˜ï¼Œå¼€å§‹è¯»å–å†…å®¹...")
                    try:
                        with open(save_path, 'r', encoding='utf-8') as f:
                            file_content = f.read()
                    except UnicodeDecodeError:
                        with open(save_path, 'r', encoding='gbk') as f:
                            file_content = f.read()
                    st.info(f"æ–‡ä»¶å†…å®¹è¯»å–æˆåŠŸï¼Œé•¿åº¦: {len(file_content)} å­—ç¬¦")
                    documents = [SimpleNamespace(
                        page_content=file_content,
                        metadata={"source": uploaded_file.name}
                    )]
                    splitter = SimpleTextSplitter(
                        chunk_size=1000,
                        chunk_overlap=200
                    )
                    splits = splitter.split_documents(documents)
                    st.info(f"æ–‡æœ¬åˆ†å‰²å®Œæˆï¼Œå…± {len(splits)} ä¸ªå—")

                    # è¯¦ç»†çš„è¯Šæ–­æ—¥å¿—
                    print(f"[DEBUG page_upload] å‡†å¤‡æ·»åŠ æ–‡æ¡£å‰çš„æ£€æŸ¥:")
                    print(f"  - rag_app: {rag_app}")
                    print(f"  - rag_app is not None: {rag_app is not None}")
                    print(f"  - rag_app.vector_store: {rag_app.vector_store}")
                    print(f"  - rag_app.vector_store is not None: {rag_app.vector_store is not None}")
                    print(f"  - bool(rag_app and rag_app.vector_store): {bool(rag_app and rag_app.vector_store)}")

                    if rag_app is not None and rag_app.vector_store is not None:
                        print(f"[DEBUG page_upload] æ¡ä»¶æ»¡è¶³ï¼Œå‡†å¤‡æ·»åŠ æ–‡æ¡£")
                        texts = [doc.page_content for doc in splits]
                        metadatas = [{"source": uploaded_file.name} for _ in splits]
                        add_result = rag_app.add_documents_to_langchain(texts, metadatas)
                        if add_result:
                            st.success(f"âœ… æˆåŠŸå¤„ç†! æ·»åŠ äº† {len(splits)} ä¸ªæ–‡æœ¬å—")
                            st.info(f"å‘é‡å­˜å‚¨ç±»å‹: {type(rag_app.vector_store).__name__}")
                            st.balloons()
                        else:
                            st.error("âŒ æ–‡æœ¬æ·»åŠ åˆ°å‘é‡åº“å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—ã€‚")
                    else:
                        print(f"[DEBUG page_upload] æ¡ä»¶ä¸æ»¡è¶³ï¼Œæ— æ³•æ·»åŠ æ–‡æ¡£")
                        st.error("âŒ å‘é‡å­˜å‚¨ä»æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ·»åŠ æ–‡æœ¬ã€‚è¯·æ£€æŸ¥ç»ˆç«¯æ—¥å¿—ã€‚")
                        st.write("**è¯Šæ–­ä¿¡æ¯:**")
                        st.write(f"- rag_app å­˜åœ¨: {rag_app is not None}")
                        if rag_app:
                            st.write(f"- rag_app.vector_store å­˜åœ¨: {rag_app.vector_store is not None}")
                            st.write(f"- rag_app.vector_store ç±»å‹: {type(rag_app.vector_store)}")
                            st.write(f"- rag_app._initialized: {rag_app._initialized}")
                            st.write(f"- rag_app.embeddings å­˜åœ¨: {rag_app.embeddings is not None}")
                        st.write("**å»ºè®®:** æŸ¥çœ‹åº”ç”¨å¯åŠ¨æ—¶çš„ç»ˆç«¯è¾“å‡ºï¼ŒæŸ¥æ‰¾ [ERROR] æ¶ˆæ¯ã€‚")
                except Exception as e:
                    import traceback
                    st.error(f"å¤„ç†å¤±è´¥: {e}")
                    st.error(traceback.format_exc())

# ============ æ™ºèƒ½é—®ç­” ============
def page_qa():
    st.header("ğŸ” æ™ºèƒ½é—®ç­”")
    st.write("ä½¿ç”¨ LangChain RAG æŠ€æœ¯å¯¹æ–‡æ¡£è¿›è¡ŒæŸ¥è¯¢")

    if not LANGCHAIN_AVAILABLE:
        st.error("âŒ LangChain æœªå®‰è£…")
        return

    if not rag_app or not rag_app.vector_store:
        st.warning("âš ï¸ å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£")
        return

    query = st.text_input("æé—®", placeholder="ä¾‹å¦‚: ç™»å½•åŠŸèƒ½çš„å‚æ•°æœ‰å“ªäº›?")

    if st.button("ğŸ” ä½¿ç”¨ LangChain æœç´¢", type="primary"):
        if query:
            with st.spinner("ä½¿ç”¨ LangChain RAG æœç´¢..."):
                try:
                    import re

                    print(f"\n{'='*80}")
                    print(f"[SEARCH START] å¼€å§‹æœç´¢: '{query}'")
                    print(f"[VECTOR_STORE TYPE] {type(rag_app.vector_store).__name__}")
                    print(f"{'='*80}\n")

                    # ä½¿ç”¨ vector_store çš„ as_retriever æ–¹æ³•è·å–æ£€ç´¢å™¨
                    if hasattr(rag_app.vector_store, 'as_retriever'):
                        retriever = rag_app.vector_store.as_retriever(search_kwargs={"k": 10})
                        print(f"[RETRIEVER TYPE] {type(retriever).__name__}")

                        # å°è¯•ä¸åŒçš„è°ƒç”¨æ–¹å¼
                        docs = None
                        try:
                            # æ–¹å¼ 1: ä½¿ç”¨ invoke æ–¹æ³•
                            print(f"[RETRIEVAL METHOD] å°è¯•ä½¿ç”¨ invoke() æ–¹æ³•...")
                            docs = retriever.invoke(query)
                            print(f"[RETRIEVAL SUCCESS] invoke() æ–¹æ³•æˆåŠŸï¼Œè·å¾— {len(docs) if docs else 0} ä¸ªåˆå§‹ç»“æœ")
                        except (AttributeError, TypeError) as e:
                            print(f"[RETRIEVAL FALLBACK] invoke() å¤±è´¥: {e}, å°è¯•å…¶ä»–æ–¹æ³•...")
                            try:
                                # æ–¹å¼ 2: ä½¿ç”¨ get_relevant_documents æ–¹æ³•
                                print(f"[RETRIEVAL METHOD] å°è¯•ä½¿ç”¨ get_relevant_documents() æ–¹æ³•...")
                                docs = retriever.get_relevant_documents(query)
                                print(f"[RETRIEVAL SUCCESS] get_relevant_documents() æ–¹æ³•æˆåŠŸï¼Œè·å¾— {len(docs) if docs else 0} ä¸ªåˆå§‹ç»“æœ")
                            except (AttributeError, TypeError) as e2:
                                print(f"[RETRIEVAL FALLBACK] get_relevant_documents() å¤±è´¥: {e2}, å°è¯•ç›´æ¥è°ƒç”¨...")
                                # æ–¹å¼ 3: ç›´æ¥è°ƒç”¨ï¼ˆ__call__ æ–¹æ³•ï¼‰
                                docs = retriever(query)
                                print(f"[RETRIEVAL SUCCESS] ç›´æ¥è°ƒç”¨æˆåŠŸï¼Œè·å¾— {len(docs) if docs else 0} ä¸ªåˆå§‹ç»“æœ")

                        if docs:
                            # ===== åå¤„ç†ï¼šåŸºäºå…³é”®è¯åŒ¹é…é‡æ–°æ’åº =====
                            print(f"\n[POST-PROCESSING] å¼€å§‹åå¤„ç†æœç´¢ç»“æœ...")

                            # æå–æŸ¥è¯¢è¯
                            query_lower = query.lower()
                            query_words = re.findall(r'[\w]+', query_lower)
                            query_words = [w for w in query_words if len(w) > 1]  # è¿‡æ»¤å•å­—ç¬¦

                            print(f"[KEYWORDS] æŸ¥è¯¢å…³é”®è¯: {query_words}")

                            # ä¸ºæ¯ä¸ªæ–‡æ¡£è®¡ç®—å…³é”®è¯åŒ¹é…å¾—åˆ†
                            scored_docs = []
                            for i, doc in enumerate(docs):
                                # è·å–æ–‡æ¡£å†…å®¹
                                if hasattr(doc, 'page_content'):
                                    content = doc.page_content
                                else:
                                    content = str(doc)

                                doc_text = content.lower()

                                # è®¡ç®—å…³é”®è¯åŒ¹é…
                                matched_words = 0
                                for word in query_words:
                                    # ä½¿ç”¨å•è¯è¾¹ç•ŒåŒ¹é…
                                    if re.search(r'\b' + re.escape(word) + r'\b', doc_text):
                                        matched_words += 1

                                # å…³é”®è¯åŒ¹é…ç‡ (0-1)
                                if len(query_words) > 0:
                                    keyword_score = matched_words / len(query_words)
                                else:
                                    keyword_score = 0

                                scored_docs.append({
                                    'doc': doc,
                                    'keyword_score': keyword_score,
                                    'matched_count': matched_words,
                                    'index': i
                                })

                                print(f"[DOC {i}] å…³é”®è¯åŒ¹é…: {matched_words}/{len(query_words)} = {keyword_score:.2%}")

                            # æŒ‰å…³é”®è¯åŒ¹é…å¾—åˆ†æ’åº
                            scored_docs.sort(key=lambda x: (x['keyword_score'], x['index']), reverse=True)

                            # è¿‡æ»¤ï¼šåªä¿ç•™æœ‰è‡³å°‘ä¸€ä¸ªå…³é”®è¯åŒ¹é…çš„æ–‡æ¡£
                            # æˆ–è€…å¦‚æœæ²¡æœ‰åŒ¹é…çš„æ–‡æ¡£ï¼Œä¿ç•™æœ€é«˜åˆ†çš„ä¸€ä¸ª
                            filtered_docs = [d for d in scored_docs if d['matched_count'] > 0]

                            if not filtered_docs and scored_docs:
                                # å¦‚æœå®Œå…¨æ²¡æœ‰å…³é”®è¯åŒ¹é…ï¼Œä¿ç•™æœ€é«˜åˆ†çš„ä¸€ä¸ª
                                print(f"[WARNING] æ²¡æœ‰å…³é”®è¯åŒ¹é…çš„æ–‡æ¡£ï¼Œä¿ç•™å¾—åˆ†æœ€é«˜çš„ç»“æœ")
                                filtered_docs = [scored_docs[0]]

                            print(f"\n[RESULT] è¿‡æ»¤åçš„æ–‡æ¡£æ•°: {len(filtered_docs)}")

                            if filtered_docs:
                                print(f"[SEARCH RESULT] æœç´¢æˆåŠŸï¼Œæ˜¾ç¤º {len(filtered_docs)} ä¸ªç›¸å…³æ–‡æ¡£\n")
                                st.markdown("### ğŸ“š æ£€ç´¢åˆ°çš„æ–‡æ¡£")
                                for idx, scored_doc in enumerate(filtered_docs, 1):
                                    doc = scored_doc['doc']
                                    keyword_score = scored_doc['keyword_score']

                                    # å¤„ç†ä¸åŒçš„è¿”å›æ ¼å¼
                                    if hasattr(doc, 'page_content'):
                                        content = doc.page_content
                                        metadata = doc.metadata if hasattr(doc, 'metadata') else {}
                                        print(f"[DOCUMENT {idx}] æ¥æº: {metadata}, åŒ¹é…åº¦: {keyword_score:.0%}")
                                    else:
                                        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
                                        content = str(doc)
                                        metadata = {}
                                        print(f"[DOCUMENT {idx}] (å­—ç¬¦ä¸²æ ¼å¼), åŒ¹é…åº¦: {keyword_score:.0%}")

                                    with st.expander(f"æ–‡æ¡£ {idx} (åŒ¹é…åº¦: {keyword_score:.0%})"):
                                        st.write(content[:500])
                                        if metadata:
                                            st.write(f"æ¥æº: {metadata}")
                            else:
                                print(f"[SEARCH RESULT] æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ–‡æ¡£")
                                st.info(f"æœªæ‰¾åˆ°ä¸'{query}'ç›¸å…³çš„æ–‡æ¡£")
                        else:
                            print(f"[SEARCH RESULT] æ£€ç´¢å™¨æœªè¿”å›ç»“æœ")
                            st.info("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
                    else:
                        st.error("å‘é‡å­˜å‚¨ä¸æ”¯æŒæ£€ç´¢æ“ä½œ")
                        print(f"[ERROR] å‘é‡å­˜å‚¨ä¸æ”¯æŒæ£€ç´¢æ“ä½œ")

                    print(f"\n{'='*80}")
                    print(f"[SEARCH END] æœç´¢å®Œæˆ")
                    print(f"{'='*80}\n")

                except Exception as e:
                    print(f"[ERROR SEARCH] æœç´¢å¤±è´¥: {e}")
                    import traceback
                    print(traceback.format_exc())
                    st.error(f"æœç´¢å¤±è´¥: {e}")


# ============ ç³»ç»Ÿä¿¡æ¯ ============
def page_rag_info():
    st.header("â„¹ï¸ RAG ç³»ç»Ÿä¿¡æ¯")

    st.markdown("## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„")
    st.markdown("""
    ```
    ç”¨æˆ·è¾“å…¥
        â†“
    LangChain æ–‡æœ¬åˆ†å‰²
        â†“
    Sentence Transformers æ–‡æœ¬åµŒå…¥
        â†“
    ChromaDB å‘é‡å­˜å‚¨
        â†“
    LangChain Retriever æ£€ç´¢
        â†“
    ç»“æ„åŒ–è¾“å‡º
    ```
    """)

    st.markdown("## ğŸ“Š ç»„ä»¶çŠ¶æ€")
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("LangChain", "âœ…" if LANGCHAIN_AVAILABLE else "âŒ")
    col2.metric("HuggingFace åµŒå…¥", "âœ…" if (rag_app and rag_app.embeddings) else "âŒ")
    col3.metric("ChromaDB å‘é‡å­˜å‚¨", "âœ…" if (rag_app and rag_app.vector_store) else "âŒ")
    col4.metric("QA é“¾", "âœ…" if (rag_app and rag_app.qa_chain) else "âš ï¸")

    st.markdown("## ğŸ› ï¸ æŠ€æœ¯æ ˆè¯¦æƒ…")
    tech_details = {
        "**Python**": "ç¼–ç¨‹è¯­è¨€",
        "**Streamlit**": "Web UI æ¡†æ¶",
        "**LangChain**": "LLM åº”ç”¨æ¡†æ¶ â­ (å·²é›†æˆ)",
        "**Sentence Transformers**": "æ–‡æœ¬åµŒå…¥æ¨¡å‹",
        "**ChromaDB**": "å‘é‡æ•°æ®åº“",
        "**RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ)**": "æ ¸å¿ƒæŠ€æœ¯æ¶æ„"
    }

    for tech, desc in tech_details.items():
        st.write(f"{tech}: {desc}")

    st.success("âœ… å®Œæ•´çš„ LangChain + RAG å®ç°")

# ============ ä¸»å‡½æ•° ============
def main():
    init_session_state()

    st.sidebar.title("ğŸ§ª RAG æ™ºèƒ½æµ‹è¯•åŠ©æ‰‹")
    st.sidebar.markdown("---")

    st.sidebar.markdown("### ğŸ› ï¸ æŠ€æœ¯æ ˆ")
    col1, col2 = st.sidebar.columns(2)
    col1.write("**LangChain**")
    col1.write("âœ…" if LANGCHAIN_AVAILABLE else "âŒ")
    col2.write("**ChromaDB**")
    col2.write("âœ…" if (rag_app and rag_app.vector_store) else "âš ï¸")

    st.sidebar.write("**Sentence Transformers**")
    st.sidebar.write("âœ…" if (rag_app and rag_app.embeddings) else "âš ï¸")

    st.sidebar.markdown("---")

    page = st.sidebar.radio(
        "å¯¼èˆª",
        [
            "ğŸ  é¦–é¡µ",
            "âœ¨ ç”Ÿæˆç”¨ä¾‹",
            "ğŸ“¤ ä¸Šä¼ æ–‡æ¡£",
            "ğŸ” æ™ºèƒ½é—®ç­”",
            "â„¹ï¸ ç³»ç»Ÿä¿¡æ¯"
        ]
    )

    st.sidebar.markdown("---")
    if LANGCHAIN_AVAILABLE:
        st.sidebar.success("âœ… LangChain å·²å¯ç”¨")
    else:
        st.sidebar.warning("âš ï¸ LangChain æœªå¯ç”¨")

    st.sidebar.info("ç‰ˆæœ¬: RAG v5.0 (LangChain)")

    if page == "ğŸ  é¦–é¡µ":
        page_home()
    elif page == "âœ¨ ç”Ÿæˆç”¨ä¾‹":
        page_generate()
    elif page == "ğŸ“¤ ä¸Šä¼ æ–‡æ¡£":
        page_upload()
    elif page == "ğŸ” æ™ºèƒ½é—®ç­”":
        page_qa()
    elif page == "â„¹ï¸ ç³»ç»Ÿä¿¡æ¯":
        page_rag_info()

if __name__ == "__main__":
    main()
